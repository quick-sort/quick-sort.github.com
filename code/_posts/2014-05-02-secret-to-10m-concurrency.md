---
layout: post
title: "如何做到千万并发连接"
description: "如何做到千万并发连接 - 内核是解决问题所在"
tags: [可伸缩性, C10M, scalability]
keywords: [C10M, 可伸缩性, scalability]
---

[C10K问题](http://www.kegel.com/c10k.html)已经是很遥远的问题了，
现在我们面对的是如何解决千万并发连接的[C10M问题](http://c10m.robertgraham.com)。
你可能认为这是不可能的，但利用那些最根本而又不为人知的技术，C10M已经不是问题。

Robert Graham在2013年的一场精彩的演讲深刻的阐述了这个问题。Unix系统最开始并不是
被设计成一个通用服务器系统，它最初是被设计成电话网络的控制系统。电话网络是实际
传输数据的，而不是Unix操作系统。所以控制层面和数据层面分开的十分清楚。我们现在
的问题是Unix系统被部分用在数据层面上，而这是有问题的。单应用单服务器的内核和多
用户内核设计起来是非常不一样的。

所以Robert说，解决C10M问题，关键在于理解：*内核不是解决问题，而正是问题所在。*

**不要让内核做繁重的操作。**

把数据包处理，内存管理，进程调度拿出内核，放到应用中去完成，这样能更加高效。
让Linux系统处理控制层面，让应用处理数据。

这样一来，对于10M并发连接，系统只用花200个时钟周期处理数据包，140,000时钟周期处理
应用逻辑。因为主存的数据访问就需要300个时钟周期，最小化代码和减少缓存失效是设计关键。

面向数据层面的系统每秒可以处理10M个数据包，面向控制层面的系统只能处理1M个数据包。

如果你觉得10M个数据包很极端，那么记住：*可伸缩性就是高度定制化* 想要做一些非同寻常
的事情，不能把性能问题外放到操作系统去处理。应用必须要自己亲自做。

**C10K问题 - 过去十年**

十年前，工程师们解决了C10K问题，让服务器能处理上万的并发连接。修改系统内核，把Apache
那样的多线程模型改成Nginx和Node那样的事件驱动模型。

*Apache问题*

- Apache问题是并发连接越多，性能越差。

- 性能与可伸缩是两个不同的概念。当人们在谈论大规模时，他们往往是在谈论性能，但他们中间有
明显的不同。

- 对于只持续几秒的短连接来说，快速事物，如果每秒执行1k个事物，那么就会有1k个并发连接。

- 如果事物的持续时间变成了10s, 那么每秒执行1k个事物，那就会有10k个连接打开。Apache 
的性能会明显下降。

- 如果当前能处理5k个并发连接每秒，你想处理10k要怎么做？升级硬件，加倍处理器速度，然后呢？
性能增加了一倍，但是规模并没有提升一倍，可能只处理到6k。即使性能提升了16倍，可能规模还是没有
到10k。这就是为什么说性能不等于规模。

- 问题在于Apache会产生一个CGI进程执行然后中止掉，这种做法并不具备可伸缩性。

- 为什么服务器不能处理10K的并发连接？因为内核中使用了一个O(n^2)的算法

	- 内核的两个基本问题：

		- 连接数 = 线程或进程数。 收到一个数据包，内核要遍历10K个进程来决定这个数据包属于哪个进程。

		- 连接数 = select/pool(单线程)。 同样的问题，收到每个数据包，都要遍历所有的socket。

	- 解决办法：让内核在常数时间内完成

		- 无论线程数多少，线程都能做到在常数时间内的切换。

		- 使用了新的epoll()/IOCompletionPort来实现常数时间内查找

- 线程调度仍然不能具备可伸缩性， 所以服务器端使用了epoll来提高可伸缩性，从而发展成Node和Nginx
那种异步编程模型。这将软件性能提升到一个新的高度。即使是一个性能平平的服务器，并发数增多时，
性能也不会大幅下降。

**C10M问题 - 下一个十年**

在不久的将来，服务器端就要面临数以百万计的并发连接。随着IPv6的使用，潜在连接数的大幅增长，
我们要达到一个新的可伸缩性的级别。

- 需要达到这种可伸缩性级别的例子：IDS/IPS（入侵检测和防御系统），DNS根服务器，TOR节点，视频流，
负载均衡，网页缓存，防火墙，电邮，垃圾过滤。

- 通常人们面对互联网伸缩性问题时看到的是硬件设备，而不是服务器系统本身的问题。因为他们销售的是
硬件加软件。买设备装到数据中心。这些设备包含主板或网络处理器，加密或数据包检测芯片等。

**C10M问题意味着什么？**

1. 10M并发连接

2. 1M/s 个连接，每个连接持续10s

3. 10Gbps/s网络速度，高速互联网入口

4. 10M个数据包每秒， 当前服务器能处理大概50K个数据包每秒，这里需要有大幅的提升。服务器一般能处理
100K个中断每秒，每一个数据包都会产生一次中断。

5. 10微秒延迟，当前强大的服务器可能能达到这样的速度，但延迟会大一些。

6. 10微秒时钟信号周期，限制了最大延迟时间

7. 10核CPU，代码应该能很好的运行在更多的CPU上。通常代码只能很好的适应到4核。服务器硬件可能要增加
更多CPU核数，软件应该也能支持更多核的机器。

**我们了解Unix，但并不了解网络编程**

- 一代的工程师都是通过读W.Richard Stevens的<<Unix Networking Programming>>学习网络编程的。但这本
书的重点是Unix，而不是网络编程。它告诉你让Unix系统完成繁重的任务，你只是写一个运行在Unix系统上的小服务。
但是内核并不是可伸缩的。所以要把繁重的操作移出内核，自己处理。

- 另一个这种影响的例子，考虑Apache的多线程处理模型。这意味着收到什么数据决定了线程调度器调用哪一个read()
*你在把线程调度系统当作数据包分发系统来用*

- Nginx的做法是不把线程调度系统当作数据包分发系统。自己来处理数据包分发。用select来找对应的socket，
因为这个socket已经有数据了，我们就可以直接读数据，而不被阻塞。

- 结论：让Unix处理网络部分，从此以外都自己处理。

**如何写出具如此伸缩性的代码**

如何改写你的代码更具有伸缩性？我们需要知道处理能力的性能实际如何。为了解决这个问题，我们需要解决：

1. 数据包处理可伸缩性

2. 多核处理可伸缩性

3. 内存可伸缩性

**数据包处理 - 自己写网络驱动**

- 数据包的问题在于，数据包要传到Unix内核。网络协议栈很复杂并且消率低下。数据包应该更直接的被传到应用中。
所以不要让系统处理数据包。

- 要达到这一点，就要自己写却动。驱动要做的事情就是直接把数据包发到你的应用而不是系统的网络协议栈。你可以
找到的驱动：PF_RING, Netmap, Intel DPDK. Intel的驱动是闭源的，但也可以找到很多支持的文档。

- 有多块？Intel曾经做过一次测试，在一个普通轻型服务器上，每秒处理80M个数据包（每个数据包花费200个时钟周期）。
普通的Linux只能做到不超过1M个数据包，这个定制驱动的性能比是80:1

- 要达到10M个数据包的目标，如果200个时钟周期花在数据包上，那么还剩1400个时钟周期在实现DNS/IDS这种类似的功能上。

- 用PF_RING，你获取到了原本的数据包，那么要自己实现TCP栈。有人正在实现内核用户态TCP栈。Intel已经提供了一个高性能的TCP栈

**多核处理**

多核可伸缩性并不是多线程可伸缩性。处理器变多的速度要大于变快的速度。多数的代码不能充分利用超过4核的处理器。
增加多核并不意味着性能更高，可能因为糟糕的软件，速度反而变慢。我们希望软件的性能能和处理器个数成线性增长。

多线程编程不是多核编程

- 多线程：

	- 一个处理器上运行多个线程

	- 通过锁协调多个线程

	- 每个线程执行不同的任务

- 多核：

	- 一个处理器上运行一个线程

	- 当两个线程（核）访问同样的数据时，他们会停止并等待

	- 多线程执行相同的任务

- 我们的问题是如何让一个应用分散到多核上

- 锁是实现在Unix内核中，在4核上使用锁，大部分软件会等待其它线程释放锁。内核会占用的性能开销会大于多核产生的性能提升。

- 我们需要的不是用红绿灯控制的十字路口，而是没有红绿灯的高架立交桥。没有等待，让每个核按照自己的速率执行。

- 解决方法：

	- 在每个核上维持自己的数据结构，然后在所有这些结构上做聚合。

	- 原子操作。CPU指令级别支持的原子操作，开销不小，所以不要妄想能解决所有问题。

		- 注意ABA问题 

		- 内存模型问题，X86和ARM有着不同的内存模型，代码会出现齐葩情况。
		
	- 无锁数据结构。线程永远不会被锁挡住。不要自己实现，跨平台无锁数据结构很复杂。

	- 线程模型。 管道模型和工人模型，线程可能是接力或各做一块再整合，而这中间不仅仅是要做线程同步。

	- 处理器粘性

		- maxcpu=2, 让操作系统使用前两个核

		- pthread_setaffinity_np 让你的线程使用其它的核

		- /proc/irq/smp_affinity 配置分别由哪些核处理哪些中断

- 内存：

	- 如果一个连接用2K主存，那么10M连接就是20G主存。然后你只有20M L3缓存，这远远不够用。花费300个CPU时钟去访问主存，
	而CPU这时是空闲着。

	- 考虑我们只有1400个时钟的预算来处理一个数据包，200个用于处理数据包本身，那么我们只有4次缓存失效的机会。

	- 数据连续性

		-	不要用指针把数据放的到处都是。每次根据指针读取数据，就是导致一次缓存失效，然后级联产生缓存失效。
			[hash pointer]->[Task Control Block]->[Socket]->[App]
		- 保存所有数据在一块内存中[TCB|Socket|App], 这只会产生一次缓存失效

	- 内存分页

		- 32G内存的页表大小为64M，这显然放不进缓存，那就产生两次缓存失效，一次页表本身，一次页表指向的内存。

		- 解决办法，设置启动项使用大分页，2M页大小，而不是4K

	- 压缩数据

		- 使用bit而不是int	

		- 索引只使用1-2字节，而不是8字节内存指针。使用缓存高效结构而不是有多次内存访问的二分查找树。

	- NUMA（非均匀内存访问模型）会加倍主存访问时间。

	- 内存池

		- 启动时预先申请所有内存

		- 每个对象，每个线程，每个socket设置缓存

		- 防止资源耗尽

	- 预先读取 

		- 同时处理两个数据包

		- 预先读取下一个Hash表项

	- 超线程

		- 网络处理器可以超线程到4，Intel只能到2
